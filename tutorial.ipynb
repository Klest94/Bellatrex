{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Bellatrex\n",
    "\n",
    "After making sure that the needed packages are installed, we can dive into the `tutorial.py` code.\n",
    "\n",
    "## Step 1: import libraries and set parameters\n",
    "\n",
    "Import the required libraries and set the parameters for the grid search, data folder paths, and other configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # avoids memory leak UserWarning caused by KMeans\n",
    "import pandas as pd\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from utilities import score_method, output_X_y\n",
    "from utilities import format_targets, format_RF_preds\n",
    "#from plot_tree_patch import plot_tree_patched\n",
    "\n",
    "from LocalMethod_class import Bellatrex\n",
    "\n",
    "# reduce MAX_TEST_SIZE for quick code testing\n",
    "MAX_TEST_SIZE = 10 #if set >= 100, it takes the (original) value X_test.shape[0]\n",
    "\n",
    "p_grid = {\n",
    "    \"n_trees\": [0.2, 0.5, 0.8],\n",
    "    \"n_dims\": [2, 5, None],\n",
    "    \"n_clusters\": [1, 2, 3]\n",
    "    }\n",
    "\n",
    "##########################################################################\n",
    "root_folder = os.getcwd()\n",
    "\n",
    "data_folder = os.path.join(root_folder, \"datasets\")\n",
    "\n",
    "''' choose appropriate learning task wth SETUP parameter '''\n",
    "SETUP = \"mtr\" # \"bin\", or \"mtr\" \n",
    "\n",
    "VERBOSE = 3\n",
    "\n",
    "PLOT_GUI = False\n",
    "'''  levels of verbosity in this script:\n",
    "    - >= 1.0: print best params, their achieved fidelity,\n",
    "              and the scoring method used to compute such performance\n",
    "    - >= 2.0 print final tree idx cluster sizes\n",
    "    - >= 3.0: plot representation of the extracted trees (two plots)\n",
    "    - >= 4.0 plot trees with GUI (if PLOT_GUI == True)\n",
    "    - >= 4.0 plot trees without GUI (if PLOT_GUI == False)\n",
    "    - >= 5.0: print params and performance during GridSearch\n",
    "'''\n",
    "\n",
    "# running different RFs or different performance measures according to the \n",
    "# prediction scenarios. So far we have implemented the following 5 cases:\n",
    "binary_key_list = [\"bin\", \"binary\"]\n",
    "survival_key_list = [\"surv\", \"survival\"]\n",
    "multi_label_key_list = [\"multi\", \"multi-l\", \"multi-label\", \"mtc\"]\n",
    "regression_key_list = [\"regression\", \"regress\", \"regr\"]\n",
    "mt_regression_key_list = [\"multi-target\", \"multi-t\", \"mtr\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and preprocess Data\n",
    "\n",
    "Load training and testing data from the `.csv` files, split them into features (X) and targets (y), and preprocess the data by formatting the target variables according to the prediction scenarios. Instantiate the appropriate `RandomForest` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_folder, SETUP + '_tutorial_train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(data_folder, SETUP + '_tutorial_test.csv'))\n",
    "\n",
    "X_train, y_train = output_X_y(df_train, SETUP)\n",
    "X_test, y_test = output_X_y(df_test, SETUP)\n",
    "\n",
    "X_train = X_train.drop(\"Unnamed: 0\", axis=1, errors=\"ignore\", inplace=False)\n",
    "X_test = X_test.drop(\"Unnamed: 0\", axis=1, errors=\"ignore\", inplace=False)\n",
    "\n",
    "assert X_train.isnull().sum().sum() < 1 #make sure there are no null values\n",
    "assert X_test.isnull().sum().sum() < 1 #make sure there are no null values\n",
    "\n",
    "# for quick testing, set a small MAX_TEST_SIZE\n",
    "X_test = X_test[:MAX_TEST_SIZE]\n",
    "y_test = y_test[:MAX_TEST_SIZE]\n",
    "\n",
    "orig_n_labels = y_test.shape[1] #meaningful only in multi-output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set target variable to correct format depending on the prediciton scenarios.E.g. set np.recarray fo survival data, or normalise data in case of single and multi-target regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = format_targets(y_train, y_test, SETUP, VERBOSE)\n",
    "\n",
    "\n",
    "### instantiate original R(S)F estimator\n",
    "if SETUP.lower() in survival_key_list:\n",
    "    rf = RandomSurvivalForest(n_estimators=100, min_samples_split=10,\n",
    "                              random_state=0)\n",
    "\n",
    "elif SETUP.lower() in binary_key_list + multi_label_key_list:\n",
    "    rf = RandomForestClassifier(n_estimators=100, min_samples_split=5,\n",
    "                                random_state=0)\n",
    "    \n",
    "elif SETUP.lower() in regression_key_list + mt_regression_key_list:\n",
    "    rf = RandomForestRegressor(n_estimators=100, min_samples_split=5,\n",
    "                               random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Instantiate and fit the Model\n",
    "\n",
    "Once the Random Forest is instantiated, the `fit` method in Bellatrex trains the Random Forest and set the parameters for Bellatrex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit RF here. The hyperparameters are given      \n",
    "Bellatrex_fitted = Bellatrex(rf, SETUP,\n",
    "                            p_grid=p_grid,\n",
    "                            proj_method=\"PCA\",\n",
    "                            dissim_method=\"rules\",\n",
    "                            feature_represent=\"weighted\",\n",
    "                            n_jobs=1,\n",
    "                            verbose=VERBOSE,\n",
    "                            plot_GUI=PLOT_GUI).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# store, for every sample in the test set, the predictions from the\n",
    "# local method and the original R(S)F\n",
    "N = min(X_test.shape[0], MAX_TEST_SIZE)        \n",
    "y_pred = []\n",
    "\n",
    "stored_info = [] #store extra info such as optimal hyperparameters (for each instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Make predictions, output explanations\n",
    "\n",
    "Loop through the test set, make predictions using the Bellatrex local method, and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N): #for every sample in the test set: call .predict\n",
    "          \n",
    "    # call the .predict method. The hyperparamters were given in the .fit.\n",
    "    # Now they are actively used and tuned for every instance\n",
    "    '''\n",
    "    the .predict ouputs:\n",
    "        - the local prediction \n",
    "        - information about the Bellatrex instance: optimal parameters,\n",
    "                    final extracted trees/rules, their weight in the prediction, etc... \n",
    "    \n",
    "    '''\n",
    "    y_local_pred, sample_info = Bellatrex_fitted.predict(X_test, i)  #tuning is also done within the .predict method\n",
    "    \n",
    "    # append all test sample predictions in y_pred\n",
    "    y_pred.append(y_local_pred) # store for debuggind and analysis\n",
    "    \n",
    "        \n",
    "y_ens_pred = format_RF_preds(rf, X_test, SETUP)\n",
    "\n",
    "# adapt to numpy array (N, L) where N = samples, L = labels (if L>1)\n",
    "#y_ens_pred = np.transpose(np.array(y_ens_pred)[:,:,1])\n",
    "if SETUP.lower() not in multi_label_key_list + mt_regression_key_list :\n",
    "    y_pred = np.array(y_pred).ravel() #force same array format\n",
    "          \n",
    "# sometimes the y_pred is not an array as expected, here we gauranatee it is\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "#in case of quick testing with few samples (less than 100)\n",
    "y_test = y_test[:MAX_TEST_SIZE]\n",
    "y_ens_pred = y_ens_pred[:MAX_TEST_SIZE]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
